{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92778525",
   "metadata": {},
   "source": [
    "# Assignment 2: Linear Models and Validation Metrics (30 marks total)\n",
    "### Due: October 10 at 11:59pm\n",
    "\n",
    "### Name: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce31b39a",
   "metadata": {},
   "source": [
    "### In this assignment, you will need to write code that uses linear models to perform classification and regression tasks. You will also be asked to describe the process by which you came up with the code. More details can be found below. Please cite any websites or AI tools that you used to help you with this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c6de86",
   "metadata": {},
   "source": [
    "## Part 1: Classification (14.5 marks total)\n",
    "\n",
    "You have been asked to develop code that can help the user determine if the email they have received is spam or not. Following the machine learning workflow described in class, write the relevant code in each of the steps below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3c6fc8",
   "metadata": {},
   "source": [
    "### Step 0: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33f86925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9d33a8",
   "metadata": {},
   "source": [
    "### Step 1: Data Input (1 mark)\n",
    "\n",
    "The data used for this task can be downloaded using the yellowbrick library: \n",
    "https://www.scikit-yb.org/en/latest/api/datasets/spam.html\n",
    "\n",
    "Use the yellowbrick function `load_spam()` to load the spam dataset into the feature matrix `X` and target vector `y`.\n",
    "\n",
    "Print the size and type of `X` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33583c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X: (4600, 57)\n",
      "Size of y: (4600,)\n",
      "\n",
      "Type X: <class 'pandas.core.frame.DataFrame'>\n",
      "Data Types X:\n",
      "word_freq_make                float64\n",
      "word_freq_address             float64\n",
      "word_freq_all                 float64\n",
      "word_freq_3d                  float64\n",
      "word_freq_our                 float64\n",
      "word_freq_over                float64\n",
      "word_freq_remove              float64\n",
      "word_freq_internet            float64\n",
      "word_freq_order               float64\n",
      "word_freq_mail                float64\n",
      "word_freq_receive             float64\n",
      "word_freq_will                float64\n",
      "word_freq_people              float64\n",
      "word_freq_report              float64\n",
      "word_freq_addresses           float64\n",
      "word_freq_free                float64\n",
      "word_freq_business            float64\n",
      "word_freq_email               float64\n",
      "word_freq_you                 float64\n",
      "word_freq_credit              float64\n",
      "word_freq_your                float64\n",
      "word_freq_font                float64\n",
      "word_freq_000                 float64\n",
      "word_freq_money               float64\n",
      "word_freq_hp                  float64\n",
      "word_freq_hpl                 float64\n",
      "word_freq_george              float64\n",
      "word_freq_650                 float64\n",
      "word_freq_lab                 float64\n",
      "word_freq_labs                float64\n",
      "word_freq_telnet              float64\n",
      "word_freq_857                 float64\n",
      "word_freq_data                float64\n",
      "word_freq_415                 float64\n",
      "word_freq_85                  float64\n",
      "word_freq_technology          float64\n",
      "word_freq_1999                float64\n",
      "word_freq_parts               float64\n",
      "word_freq_pm                  float64\n",
      "word_freq_direct              float64\n",
      "word_freq_cs                  float64\n",
      "word_freq_meeting             float64\n",
      "word_freq_original            float64\n",
      "word_freq_project             float64\n",
      "word_freq_re                  float64\n",
      "word_freq_edu                 float64\n",
      "word_freq_table               float64\n",
      "word_freq_conference          float64\n",
      "char_freq_;                   float64\n",
      "char_freq_(                   float64\n",
      "char_freq_[                   float64\n",
      "char_freq_!                   float64\n",
      "char_freq_$                   float64\n",
      "char_freq_#                   float64\n",
      "capital_run_length_average    float64\n",
      "capital_run_length_longest      int64\n",
      "capital_run_length_total        int64\n",
      "dtype: object\n",
      "\n",
      "Type y: <class 'pandas.core.series.Series'>\n",
      "Data Types y:\n",
      "int64\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Import spam dataset from yellowbrick library\n",
    "from yellowbrick.datasets import load_spam\n",
    "\n",
    "X, y = load_spam()\n",
    "#print(X)\n",
    "#print(y)\n",
    "\n",
    "# TO DO: Print size and type of X and y\n",
    "\n",
    "print(\"Size of X: {}\".format(X.shape))\n",
    "print(\"Size of y: {}\\n\".format(y.shape))\n",
    "\n",
    "print(\"Type X: {}\\nData Types X:\\n{}\\n\".format(type(X), X.dtypes))\n",
    "print(\"Type y: {}\\nData Types y:\\n{}\".format(type(y), y.dtypes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156db208",
   "metadata": {},
   "source": [
    "### Step 2: Data Processing (1.5 marks)\n",
    "\n",
    "Check to see if there are any missing values in the dataset. If necessary, select an appropriate method to fill-in the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e7204f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Null X: 0\n",
      "Total Null y: 0\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Check if there are any missing values and fill them in if necessary\n",
    "total_null_X = np.sum(X.isna().sum())\n",
    "total_null_y = y.isna().sum()\n",
    "print(\"Total Null X: {}\".format(total_null_X))\n",
    "print(\"Total Null y: {}\".format(total_null_y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a489285a",
   "metadata": {},
   "source": [
    "For this task, we want to test if the linear model would still work if we used less data. Use the `train_test_split` function from sklearn to create a new feature matrix named `X_small` and a new target vector named `y_small` that contain **5%** of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9bc4a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Create X_small and y_small \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_small, X_small_test1, y_small, y_small_test = train_test_split(X, y, train_size=0.05, random_state=0)\n",
    "#print(X_small.shape)\n",
    "#print(y_small)\n",
    "X_small_train, X_small_test2, y_small_train, y_small_test2 = train_test_split(X_small, y_small, random_state=0)\n",
    "\n",
    "X_full_train, X_full_test, y_full_train, y_full_test = train_test_split(X, y, stratify=y, random_state=0)\n",
    "\n",
    "#print(X.iloc[:,:2])\n",
    "X_2c_train, X_2c_test, y_2c_train, y_2c_test = train_test_split(X.iloc[:,:2], y, random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e6c46f",
   "metadata": {},
   "source": [
    "### Step 3: Implement Machine Learning Model\n",
    "\n",
    "1. Import `LogisticRegression` from sklearn\n",
    "2. Instantiate model `LogisticRegression(max_iter=2000)`.\n",
    "3. Implement the machine learning model with three different datasets: \n",
    "    - `X` and `y`\n",
    "    - Only first two columns of `X` and `y`\n",
    "    - `X_small` and `y_small`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fef4adef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=2000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=2000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=2000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model_A = LogisticRegression(max_iter=2000)\n",
    "model_B = LogisticRegression(max_iter=2000)\n",
    "model_C = LogisticRegression(max_iter=2000)\n",
    "\n",
    "model_A.fit(X_full_train, y_full_train)\n",
    "model_B.fit(X_2c_train, y_2c_train)\n",
    "model_C.fit(X_small_train, y_small_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89f3d84",
   "metadata": {},
   "source": [
    "### Step 4: Validate Model\n",
    "\n",
    "Calculate the training and validation accuracy for the three different tests implemented in Step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04f2f197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy for `X` and `y` = 0.93362\n",
      "Val Accuracy for `X` and `y` = 0.93130\n",
      "Training Accuracy for Only first two columns of `X` and `y` = 0.60841\n",
      "Val Accuracy for Only first two columns of `X` and `y` = 0.61304\n",
      "Training Accuracy for `X_small` and `y_small` = 0.93605\n",
      "Val Accuracy for `X_small` and `y_small` = 0.93103\n"
     ]
    }
   ],
   "source": [
    "A_train = model_A.score(X_full_train, y_full_train)\n",
    "A_val = model_A.score(X_full_test, y_full_test)\n",
    "\n",
    "print(\"Training Accuracy for `X` and `y` = {:.5f}\".format(A_train))\n",
    "print(\"Val Accuracy for `X` and `y` = {:.5f}\".format(A_val))\n",
    "\n",
    "B_train = model_B.score(X_2c_train, y_2c_train)\n",
    "B_val = model_B.score(X_2c_test, y_2c_test)\n",
    "\n",
    "print(\"Training Accuracy for Only first two columns of `X` and `y` = {:.5f}\".format(B_train))\n",
    "print(\"Val Accuracy for Only first two columns of `X` and `y` = {:.5f}\".format(B_val))\n",
    "\n",
    "C_train = model_C.score(X_small_train, y_small_train)\n",
    "C_val = model_C.score(X_small_test2, y_small_test2)\n",
    "\n",
    "print(\"Training Accuracy for `X_small` and `y_small` = {:.5f}\".format(C_train))\n",
    "print(\"Val Accuracy for `X_small` and `y_small` = {:.5f}\".format(C_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352106a3",
   "metadata": {},
   "source": [
    "### Step 5: Visualize Results (4 marks)\n",
    "\n",
    "1. Create a pandas DataFrame `results` with columns: Data size, training accuracy, validation accuracy\n",
    "2. Add the data size, training and validation accuracy for each dataset to the `results` DataFrame\n",
    "3. Print `results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be4b5c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Data size       Shape  training accuracy  validation accuracy\n",
      "0     262200  (4600, 57)           0.933623             0.931304\n",
      "1       9200   (4600, 2)           0.608406             0.613043\n",
      "2      13110   (230, 57)           0.936047             0.931034\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE FOR STEPS 3-5\n",
    "# Note: for any random state parameters, you can use random_state = 0\n",
    "# HINT: USING A LOOP TO STORE THE DATA IN YOUR RESULTS DATAFRAME WILL BE MORE EFFICIENT\n",
    "\n",
    "columns_add = ['Data size', 'Shape', 'training accuracy', 'validation accuracy']\n",
    "\n",
    "results = pd.DataFrame(columns=columns_add)\n",
    "results['Data size'] = [X.size, X.iloc[:,:2].size, X_small.size]\n",
    "results['Shape'] = [X.shape, X.iloc[:,:2].shape, X_small.shape]\n",
    "results['training accuracy'] = [A_train, B_train, C_train]\n",
    "results['validation accuracy'] = [A_val, B_val, C_val]\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4427d4f",
   "metadata": {},
   "source": [
    "### Questions (4 marks)\n",
    "1. How do the training and validation accuracy change depending on the amount of data used? Explain with values.\n",
    "2. In this case, what do a false positive and a false negative represent? Which one is worse?\n",
    "\n",
    "*YOUR ANSWERS HERE*\n",
    "\n",
    "1. With full data, the training accuracy is 0.933623 and validation accuracy is 0.931304, which is very good. The full data enables the model to train on more data points and more features, which adds to the accuracy of the model. When only two columns are used, the data size is significantly smaller, only 9200 points vs 262200 points. Many key features may be eliminated which significantly reduces the trianing and validation accuracy. With less data of two columns, the model unfits the data. If we use 5% of the data, we keep many key features which enables the model to generate higher training and validation accuracy of 0.936047 and 0.931034. The model has good results. An important but perhaps negligible factor is that with the 5%, the training accuracy is a bit higher than full data training accuracy. With a large dataset, the model will tend to underfit the training data but the validation accuracy should increase however it is almost negligible here.\n",
    "\n",
    "2. False positive is when a non-spam email is flagged as spam. False negative is when a spam email is flagged non-spam. The worse on is false positive, since a critical or time sensitive email may go to spam or get deleted and it may have severe consequence. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7559517a",
   "metadata": {},
   "source": [
    "### Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "I used the examples posted on D2L and Lab to help with it. I wrote it myself.\n",
    "1. In what order did you complete the steps?\n",
    "I completed the steps in sequence but for some steps like Step 3 and 2 I had to go back to add more to Step 2 since Step 3 required different data sizes.\n",
    "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "I did not use generative AI but I did use online help from Google and Stackoverflow to help with some tasks.\n",
    "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?\n",
    "Yes, the questions need to be a bit more detailed. I was very confused when the Step 2 asked 5% of the data to be used for the model. I was confused if it mean 5% of the data to train and 95% to test on or did it ask to cut 5% out of the data and use that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fe687f",
   "metadata": {},
   "source": [
    "*DESCRIBE YOUR PROCESS HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4c78a8",
   "metadata": {},
   "source": [
    "## Part 2: Regression (10.5 marks total)\n",
    "\n",
    "For this section, we will be evaluating concrete compressive strength of different concrete samples, based on age and ingredients. You will need to repeat the steps 1-4 from Part 1 for this analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ba83c5",
   "metadata": {},
   "source": [
    "### Step 1: Data Input (1 mark)\n",
    "\n",
    "The data used for this task can be downloaded using the yellowbrick library: \n",
    "https://www.scikit-yb.org/en/latest/api/datasets/concrete.html\n",
    "\n",
    "Use the yellowbrick function `load_concrete()` to load the spam dataset into the feature matrix `X` and target vector `y`.\n",
    "\n",
    "Print the size and type of `X` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ff2e34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X: (1030, 8)\n",
      "Size of y: (1030,)\n",
      "\n",
      "Type X: <class 'pandas.core.frame.DataFrame'>\n",
      "Data Types X:\n",
      "cement    float64\n",
      "slag      float64\n",
      "ash       float64\n",
      "water     float64\n",
      "splast    float64\n",
      "coarse    float64\n",
      "fine      float64\n",
      "age         int64\n",
      "dtype: object\n",
      "\n",
      "Type y: <class 'pandas.core.series.Series'>\n",
      "Data Types y:\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Import spam dataset from yellowbrick library\n",
    "\n",
    "from yellowbrick.datasets import load_concrete\n",
    "\n",
    "X, y = load_concrete()\n",
    "#print(X)\n",
    "#print(y)\n",
    "\n",
    "# TO DO: Print size and type of X and y\n",
    "\n",
    "print(\"Size of X: {}\".format(X.shape))\n",
    "print(\"Size of y: {}\\n\".format(y.shape))\n",
    "\n",
    "print(\"Type X: {}\\nData Types X:\\n{}\\n\".format(type(X), X.dtypes))\n",
    "print(\"Type y: {}\\nData Types y:\\n{}\".format(type(y), y.dtypes))\n",
    "\n",
    "\n",
    "# TO DO: Print size and type of X and y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5294cfa",
   "metadata": {},
   "source": [
    "### Step 2: Data Processing (0.5 marks)\n",
    "\n",
    "Check to see if there are any missing values in the dataset. If necessary, select an appropriate method to fill-in the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "693c5fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Null X: 0\n",
      "Total Null y: 0\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Check if there are any missing values and fill them in if necessary\n",
    "\n",
    "#print(X)\n",
    "total_null_X = np.sum(X.isna().sum())\n",
    "total_null_y = y.isna().sum()\n",
    "print(\"Total Null X: {}\".format(total_null_X))\n",
    "print(\"Total Null y: {}\".format(total_null_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc60489",
   "metadata": {},
   "source": [
    "### Step 3: Implement Machine Learning Model (1 mark)\n",
    "\n",
    "1. Import `LinearRegression` from sklearn\n",
    "2. Instantiate model `LinearRegression()`.\n",
    "3. Implement the machine learning model with `X` and `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5041945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "# Note: for any random state parameters, you can use random_state = 0\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "lr_model = LinearRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de28482",
   "metadata": {},
   "source": [
    "### Step 4: Validate Model (1 mark)\n",
    "\n",
    "Calculate the training and validation accuracy using mean squared error and R2 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "970c038b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Training Accuracy 111.36\n",
      "MSE Val Accuracy 95.90\n",
      "R2 Training Accuracy 0.61082\n",
      "R2 Val Accuracy 0.62341\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "mse_train = mean_squared_error(y_train, lr_model.predict(X_train))\n",
    "mse_test = mean_squared_error(y_test, lr_model.predict(X_test))\n",
    "\n",
    "r2_train = r2_score(y_train, lr_model.predict(X_train))\n",
    "r2_test = r2_score(y_test, lr_model.predict(X_test))\n",
    "\n",
    "print(\"MSE Training Accuracy {:.2f}\".format(mse_train))\n",
    "print(\"MSE Val Accuracy {:.2f}\".format(mse_test))\n",
    "\n",
    "print(\"R2 Training Accuracy {:.5f}\".format(r2_train))\n",
    "print(\"R2 Val Accuracy {:.5f}\".format(r2_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54aa7795",
   "metadata": {},
   "source": [
    "### Step 5: Visualize Results (1 mark)\n",
    "1. Create a pandas DataFrame `results` with columns: Training accuracy and Validation accuracy, and index: MSE and R2 score\n",
    "2. Add the accuracy results to the `results` DataFrame\n",
    "3. Print `results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88d223f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Training accuracy  Validation accuracy\n",
      "MSE         111.358439            95.904136\n",
      "R2            0.610823             0.623414\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "\n",
    "results = pd.DataFrame(index = ['MSE','R2'], columns=['Training accuracy', 'Validation accuracy'])\n",
    "results['Training accuracy'] = [mse_train, r2_train]\n",
    "results['Validation accuracy'] = [mse_test, r2_test]\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a42bda",
   "metadata": {},
   "source": [
    "### Questions (2 marks)\n",
    "1. Did using a linear model produce good results for this dataset? Why or why not?\n",
    "\n",
    "The linear model generated poor results for the dataset. The traning accuracy MSE is worse than the validation accuracy MSE. The traning accuracy should mostly be better than validation accuracy. Same goes for the R2 score, which is 0.61 for training and 0.62 for validation, and both are much lower from 1. That indicates the model is poor. The model underfits the data and it is high bias, we should try other models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca0ff2f",
   "metadata": {},
   "source": [
    "### Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "I used Lab and the examples posted on D2L. I also used sklearn online information to find metrics.\n",
    "1. In what order did you complete the steps?\n",
    "I did the steps in sequence in 1, 2, 3, 4, 5.\n",
    "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "I had not used generative AI, I only used the labs and the examples posted online and it was enough to get going.\n",
    "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?\n",
    "One of the challanges is understanding what the data means. When we have results for the MSE and R2 scores, how do we know what is good and what is bad. Maybe we can try other models to compare."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdb0880",
   "metadata": {},
   "source": [
    "*DESCRIBE YOUR PROCESS HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72ac3eb",
   "metadata": {},
   "source": [
    "## Part 3: Observations/Interpretation (3 marks)\n",
    "\n",
    "Describe any pattern you see in the results. Relate your findings to what we discussed during lectures. Include data to justify your findings.\n",
    "\n",
    "The first pattern I recognized is that removing features from data can cause the accuracy to significantly decrease. When we used only 2 columns out of 57 in the spam data, our training accuracy decreased from 0.933623 to 0.608406 and validation accuracy decreased from 0.931304 to 0.613043. Another pattern I noticed was that with larger data, but keep all features, the training accuracy decreases by a bit and slightly underfits the data. The key is to keep as many features as you can since they affect the model accuracy more than just having many data points. The two columns dataset has 4600 rows (data points), but it still did not improve the model accuracy.\n",
    "\n",
    "*ADD YOUR FINDINGS HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b84eed",
   "metadata": {},
   "source": [
    "## Part 4: Reflection (2 marks)\n",
    "Include a sentence or two about:\n",
    "- what you liked or disliked,\n",
    "- found interesting, confusing, challangeing, motivating\n",
    "while working on this assignment.\n",
    "\n",
    "What I liked was that questions had some guidance to help you get started, it really helps a lot and gets people going in the correct direction instead of wasting time. What I disliked was that some questions need to be more clearly in their requirements or what detail they are asking.\n",
    "\n",
    "The way some questions are asked, and what is required for the answers is bit confusing. If the question asks where did I get some guidance, I would expect to answer it in a simple format instead of fully citing everything because it did not specifically ask for that. I also would like all information to be in the assignment, not scattered in random files here and there because that adds to confusion a lot.\n",
    "\n",
    "*ADD YOUR THOUGHTS HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db951b3a",
   "metadata": {},
   "source": [
    "## Part 5: Bonus Question (4 marks)\n",
    "\n",
    "Repeat Part 2 with Ridge and Lasso regression to see if you can improve the accuracy results. Which method and what value of alpha gave you the best R^2 score? Is this score \"good enough\"? Explain why or why not.\n",
    "\n",
    "**Remember**: Only test values of alpha from 0.001 to 100 along the logorithmic scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47623d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Ridge with the value of: 0.001\n",
      "R2 Training Accuracy 0.6108229425\n",
      "R2 Val Accuracy 0.6234144628\n",
      "\n",
      "For Ridge with the value of: 0.01\n",
      "R2 Training Accuracy 0.6108229425\n",
      "R2 Val Accuracy 0.6234144663\n",
      "\n",
      "For Ridge with the value of: 0.1\n",
      "R2 Training Accuracy 0.6108229425\n",
      "R2 Val Accuracy 0.6234145019\n",
      "\n",
      "For Ridge with the value of: 1\n",
      "R2 Training Accuracy 0.6108229424\n",
      "R2 Val Accuracy 0.6234148577\n",
      "\n",
      "For Ridge with the value of: 10\n",
      "R2 Training Accuracy 0.6108229386\n",
      "R2 Val Accuracy 0.6234184085\n",
      "\n",
      "For Ridge with the value of: 100\n",
      "R2 Training Accuracy 0.6108225614\n",
      "R2 Val Accuracy 0.6234532116\n",
      "\n",
      "For Lasso with the value of: 0.001\n",
      "R2 Training Accuracy 0.6108229422\n",
      "R2 Val Accuracy 0.6234159585\n",
      "\n",
      "For Lasso with the value of: 0.01\n",
      "R2 Training Accuracy 0.6108229210\n",
      "R2 Val Accuracy 0.6234294013\n",
      "\n",
      "For Lasso with the value of: 0.1\n",
      "R2 Training Accuracy 0.6108208014\n",
      "R2 Val Accuracy 0.6235616722\n",
      "\n",
      "For Lasso with the value of: 1\n",
      "R2 Training Accuracy 0.6106090281\n",
      "R2 Val Accuracy 0.6246688753\n",
      "\n",
      "For Lasso with the value of: 10\n",
      "R2 Training Accuracy 0.6043144042\n",
      "R2 Val Accuracy 0.6267738585\n",
      "\n",
      "For Lasso with the value of: 100\n",
      "R2 Training Accuracy 0.4675761616\n",
      "R2 Val Accuracy 0.5074125634\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(X, y, random_state=0)\n",
    "\n",
    "for i in [0.001, 0.01, 0.1, 1, 10, 100]:\n",
    "    r_model = Ridge(alpha=i).fit(X_train_new, y_train_new)\n",
    "    r2_train_new = r2_score(y_train_new, r_model.predict(X_train_new))\n",
    "    r2_test_new = r2_score(y_test_new, r_model.predict(X_test_new))\n",
    "    \n",
    "    print(\"For Ridge with the value of: {}\".format(i))\n",
    "    print(\"R2 Training Accuracy {:.10f}\".format(r2_train_new))\n",
    "    print(\"R2 Val Accuracy {:.10f}\\n\".format(r2_test_new))\n",
    "\n",
    "for i in [0.001, 0.01, 0.1, 1, 10, 100]:\n",
    "    l_model = Lasso(alpha=i).fit(X_train_new, y_train_new)\n",
    "    r2_train_new = r2_score(y_train_new, l_model.predict(X_train_new))\n",
    "    r2_test_new = r2_score(y_test_new, l_model.predict(X_test_new))\n",
    "    \n",
    "    print(\"For Lasso with the value of: {}\".format(i))\n",
    "    print(\"R2 Training Accuracy {:.10f}\".format(r2_train_new))\n",
    "    print(\"R2 Val Accuracy {:.10f}\\n\".format(r2_test_new))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b606236",
   "metadata": {},
   "source": [
    "After using multiple values including 0.001, 0.1, 1, 10 and 100, both the Ridge and Lasso models do not offer better scores. While the Ridge model training and validation accuracy scores dont significantly change as we go from 0.001 to 100, Lasso definitely seems to decrease in training and validation accuracy scores. All three models including linear regeression, ridge and lasso generate poor accuracy scores and underfit the data. We should look at other models. The linear regression model had training accuracy of 0.61082 and validation accuracy of 0.62341 which is very similar to ridge and lasso training accuracy of 0.61082 and validation accuracy of 0.62341. The value of 0.001 gave the most accurate scores.\n",
    "\n",
    "*ANSWER HERE*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 ('ensf-ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "5a29cbdb3a4d87bd4ca51dbe633eced75e23fbf7317d5f915665cf29f889d0cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
